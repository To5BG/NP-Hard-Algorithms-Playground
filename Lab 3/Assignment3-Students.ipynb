{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de74baa0",
   "metadata": {},
   "source": [
    "# Assignment 3 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d655f9ef",
   "metadata": {},
   "source": [
    "# Part 1 - Basic Probability (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc3f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49f9246b",
   "metadata": {},
   "source": [
    "### 1.0 Histogram (0.5 points)\n",
    "\n",
    "Before we start, we need a way of visualizing the data. Implement the `custom_histogram` function as follows:\n",
    "  - **Inputs**: the `data` and the number of `bins`\n",
    "  - **Output**: the number of `counts` in each bin, and the `bin_edges`, such that:\n",
    "    - The bin `counts[i]` includes values that are greater or equal to `bin_edges[i]` and smaller than `bin_edges[i+1]`.\n",
    "    - The last bin also counts the maximum value.\n",
    "    - The first bin edge is the minimum value in the data, while the last bin edge is the maximum value.\n",
    "    - The bins are equally spaced\n",
    "  - **Density Parameter**: If the `density` parameter is set to True, scale the histogram to represent a probability density function (normalize each bin by the total area of the histogram, not by the total number of elements).\n",
    "\n",
    "The output of your function (left) should be the same as that of `np.histogram()` (right).\n",
    "\n",
    "**NOTE:** You are not allowed to use any external libraries in your implementation. This is true for all exercises, unless stated otherwise. If you need to use code from a previous cell you did not implement, then you're allowed to use an outside library to replace that missing part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_histogram(data, bins, density = False):\n",
    "    \"\"\"\n",
    "    A custom function to compute histogram data without using np.histogram, with an option to scale the histogram\n",
    "    to represent a probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: array-like, the input data.\n",
    "    - bins: int, the number of bins.\n",
    "    - density: bool, if True, scale the histogram counts to represent a probability distribution.\n",
    "    \n",
    "    Returns:\n",
    "    - counts: array of the counts in each bin, or the probability densities if density=True.\n",
    "    - bin_edges: array of the bin edges.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get all values\n",
    "    min_val, max_val = min(data), max(data)\n",
    "    range_val = max_val - min_val\n",
    "    bin_width = range_val / bins\n",
    "    bin_edges = [min_val + i * bin_width for i in range(bins + 1)]\n",
    "    \n",
    "    # Initialize counts list\n",
    "    counts = [0] * bins\n",
    "    \n",
    "    # Assign all data points to bins\n",
    "    for value in data:\n",
    "        if value == max_val:\n",
    "            # Special case to include the max value in the last bin\n",
    "            counts[-1] += 1\n",
    "        else:\n",
    "            bin_index = int((value - min_val) / bin_width)\n",
    "            counts[bin_index] += 1\n",
    "    \n",
    "    if density:\n",
    "        # Scale counts to probability density\n",
    "        total_count = sum(counts)\n",
    "        counts = [count / (total_count * bin_width) for count in counts]\n",
    "    \n",
    "    return counts, bin_edges\n",
    "\n",
    "# Example usage with density=True\n",
    "samples = np.random.randn(10000)  # Generate some data\n",
    "n_bins = 10  # Number of bins\n",
    "\n",
    "# Generate histogram data with density scaling\n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density = True)\n",
    "\n",
    "# Generate reference histogram data with density scaling, using NumPy\n",
    "reference_hist, reference_bin_edges = np.histogram(samples, bins = n_bins, density = True)\n",
    "\n",
    "# Plotting the histogram as a probability distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1])\n",
    "ax[0].bar(bin_centers, hist, width = np.diff(bin_centers)[0])\n",
    "ax[0].set_xlabel('Value')\n",
    "ax[0].set_ylabel('Probability Density')\n",
    "ax[0].set_title('Custom Histogram as Probability Distribution')\n",
    "\n",
    "# Checking the answer using plt.hist()\n",
    "ax[1].hist(samples, bins = n_bins, density = True)\n",
    "ax[1].set_xlabel('Value')\n",
    "ax[1].set_ylabel('Probability Density')\n",
    "ax[1].set_title('Actual Histogram as Probability Distribution')\n",
    "plt.show()\n",
    "\n",
    "assert len(hist) == len(reference_hist) and (np.abs(hist - reference_hist) < 1e-6).all() # histogram\n",
    "assert len(bin_edges) == len(reference_bin_edges) and (np.abs(bin_edges - reference_bin_edges) < 1e-6).all() # bin edges"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d544c10",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Uniform generator (0.3 points)\n",
    "\n",
    "Construct and test your uniform number generator, by implementing the following functions:\n",
    "* ```uniform_rv(start, end)``` - returns a random sample between ```start``` and ```end``` with **uniform probability** (using only [```random.random()```](https://docs.python.org/3/library/random.html#random.random)).\n",
    "* ```uniform_rv_n(n_samples, start, end)``` - returns a list of ```n_samples``` that are drawn from a uniform distribution (using only ```uniform_rv(start, end)```).\n",
    "* ```uniform_pdf(x, start, end)``` - returns the probability for a certain ```x``` value.\n",
    "\n",
    "Note that you can also implement your own pseudo-number generator using linear congruential generator (LCG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19573a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_rv(start = 0, end = 1):\n",
    "    \"\"\"\n",
    "    Generate a single random variable that is uniformly distributed between start and end.\n",
    "\n",
    "    Parameters:\n",
    "    - start: float, the start of the interval.\n",
    "    - end: float, the end of the interval.\n",
    "    \n",
    "    Returns:\n",
    "    - float, the random variable.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return start + random.random() * (end - start)\n",
    "\n",
    "def uniform_rv_n(n_samples, start = 0, end = 1):\n",
    "    \"\"\"\n",
    "    Generate n_samples random variables that are uniformly distributed between start and end.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): the number of samples.\n",
    "    - start (float):  the start of the interval.\n",
    "    - end (float): , the end of the interval.\n",
    "\n",
    "    Returns:\n",
    "    - list of floats, the random variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [uniform_rv(start, end) for _ in range(n_samples)] \n",
    "\n",
    "def uniform_pdf(x, start, end):\n",
    "    \"\"\"\n",
    "    Calculate the probability density function (PDF) of a uniform distribution at a given point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x (float): The point at which to calculate the PDF.\n",
    "    - start (float): The start of the interval.\n",
    "    - end (float): The end of the interval.\n",
    "    Returns:\n",
    "    - float: The PMF value for the given parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if start <= x <= end: return 1 / (end - start)\n",
    "    else: return 0\n",
    "\n",
    "# Running Experiment\n",
    "n_samples = 100000\n",
    "start = -1\n",
    "end = 3\n",
    "samples = uniform_rv_n(n_samples, start = start, end = end)\n",
    "n_bins = 30\n",
    "\n",
    "# Generate histogram data with density scaling\n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density = True)\n",
    "\n",
    "# Plotting the histogram as a probability distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1]) # calculate the position of the bin ceters\n",
    "ax.bar(bin_centers, hist, width = np.diff(bin_centers)[0], label = \"Data\") # plot the histogram\n",
    "\n",
    "# Plot the density function\n",
    "xx = np.linspace(min(samples), max(samples), 10000)\n",
    "yy = [uniform_pdf(x, start, end) for x in xx] \n",
    "ax.plot(xx, yy, color = 'r', label = \"Probability density function\")\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Probability Density')\n",
    "ax.set_title(f'Uniformly Distributed Random Variables $U({start},{end})$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7714215",
   "metadata": {},
   "source": [
    "### 1.2. Bernoulli Distribution (0.3 points).\n",
    "\n",
    "Construct and test a Bernoulli random variable generator, by implementing the following functions:\n",
    "* ```bernoulli_rv(p)``` - returns a random sample from a Bernoulli distribution with parameter ```p``` (using only ```uniform_rv()```).\n",
    "* ```bernoulli_rv_n(n_samples, p)``` - returns a list of ```n_samples``` that are drawn from a Bernoulli distribution with parameter ```p``` (using only ```bernoulli_rv(p)```).\n",
    "* ```bernoulli_pdf(x, p)``` - returns the probability for a certain ```x``` value.\n",
    "\n",
    "Note that in this case we do not scale by the total area of the histogram, but by the total number of elements. Why is that?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f45f7a32",
   "metadata": {},
   "source": [
    "<div style=\"background: #f6e28b; padding: 20px; color: black;\">\n",
    "\n",
    "<b>WRITE YOUR ANSWER TO THE QUESTION ABOVE HERE (DOUBLE-CLICK TO EDIT)</b>\n",
    "\n",
    "<br> In the context of a Bernoulli Distribution, we scale by the total number of elements rather than the histogram's area because we are dealing with discrete outcomes and their probabilities, as specified by a probability mass function, not continuous variables and probability densities. This approach reflects the probabilities of discrete outcomes directly. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea46fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_rv(p):\n",
    "    \"\"\"\n",
    "    Generate a single Bernoulli-distributed random variable.\n",
    "\n",
    "    Parameters:\n",
    "    - p (float): the probability of the random variable being 1.\n",
    "\n",
    "    Returns:\n",
    "    - int: 1 with probability p, 0 with probability 1-p.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return int(uniform_rv() < p)\n",
    "\n",
    "def bernoulli_rv_n(n_samples, p):\n",
    "    \"\"\"\n",
    "    Generate n_samples Bernoulli-distributed random variables.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): the number of samples.\n",
    "    - p (float): the probability of the random variable being 1.\n",
    "\n",
    "    Returns:\n",
    "    - list of ints: the random variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [bernoulli_rv(p) for _ in range(n_samples)]\n",
    "\n",
    "def bernoulli_pmf(x, p):\n",
    "    \"\"\"\n",
    "    Calculate the probability mass function (PMF) of a Bernoulli distribution at a given point x.\n",
    "    \n",
    "    Parameters:\n",
    "    - x (int): The point at which to calculate the PMF.\n",
    "    - p (float): The probability of the random variable being 1.\n",
    "\n",
    "    Returns:\n",
    "    - float: The PMF value for the given parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    if x == 1: return p\n",
    "    elif x == 0: return 1 - p\n",
    "    else: return 0\n",
    "\n",
    "# Run Experiment\n",
    "n_samples = 100000\n",
    "p = 0.2\n",
    "samples = bernoulli_rv_n(n_samples, p) # Generate Bernoulli-distributed random variables\n",
    "n_bins = 2 # the number of bins is always 2\n",
    "\n",
    "# Generate histogram data \n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density = False) # without density scaling\n",
    "\n",
    "# Plotting the histogram as a mass function distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1]) \n",
    "ax.bar(bin_centers, np.array(hist) / n_samples, width = np.diff(bin_centers)[0], label = 'Data')\n",
    "\n",
    "# Plotting the real probability mass function\n",
    "xx = [0,1]\n",
    "yy = [bernoulli_pmf(x, p) for x in xx]\n",
    "ax.stem(xx, yy, \"r\", label = 'Probability mass function')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title(f'Bernoulli Distributed RV $Ber(p={p})$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f57465af",
   "metadata": {},
   "source": [
    "### 1.3 Binomial Distribution (0.3 points)\n",
    "\n",
    "Construct and test a Binomial random variable generator, by implementing the following functions:\n",
    "* ```binomial_rv(n, p)``` - returns a random sample from a Binomial distribution with parameters ```n``` and ```p``` (using only ```bernoulli_rv_n()```).\n",
    "* ```binomial_rv_n(n_samples, n, p)``` - returns a list of ```n_samples``` that are drawn from a Binomial distribution with parameters ```n``` and ```p``` (using only ```binomial_rv(n, p)```).\n",
    "* ```binomial_pdf(x, n, p)``` - returns the probability for a certain ```x``` value (you are allowed to use `scipy.special.comb` in your implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb\n",
    "\n",
    "def binomial_rv(p, n):\n",
    "    \"\"\"\n",
    "    Generate a single binomial-distributed random variable.\n",
    "\n",
    "    Parameters:\n",
    "    - p (float): the probability of the random variable being 1.\n",
    "    - n (int): the number of trials.\n",
    "\n",
    "    Returns:\n",
    "    - int: the number of successes in n trials.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return np.count_nonzero(bernoulli_rv_n(n, p))\n",
    "\n",
    "def binomial_rv_n(n_samples, p, n):\n",
    "    \"\"\"\n",
    "    Generate n_samples binomial-distributed random variables.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): the number of samples.\n",
    "    - p (float): the probability of the random variable being 1.\n",
    "    - n (int): the number of trials.\n",
    "\n",
    "    Returns:\n",
    "    - list of ints: the random variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [binomial_rv(p, n) for _ in range(n_samples)]\n",
    "\n",
    "\n",
    "def binomial_pmf(x, n, p):\n",
    "    \"\"\"\n",
    "    Calculate the probability mass function (PMF) of a binomial distribution at a given point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x (int): The point at which to calculate the PMF.\n",
    "    - n (int): the number of trials.\n",
    "    - p (float): the probability of the random variable being 1.\n",
    "\n",
    "    Returns:\n",
    "    - float: The PMF value for the given parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return comb(n, x) * (p ** x) * ((1 - p) ** (n - x))\n",
    "\n",
    "# Run Experiment\n",
    "p = 0.1\n",
    "n = 20\n",
    "n_samples = 25000\n",
    "samples = binomial_rv_n(n_samples, p, n) # Generate binomial-distributed random variables\n",
    "n_bins = n # we have one bin for each integer values\n",
    "\n",
    "# Generate histogram data \n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density = False) # without density scaling\n",
    "\n",
    "# Plotting the histogram as a probability mass function\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1])\n",
    "ax.bar(bin_centers, np.array(hist) / n_samples, width = np.diff(bin_centers)[0], edgecolor = 'black', label = \"Data\")\n",
    "\n",
    "# Plotting the real probability mass function\n",
    "xx = np.arange(int(min(samples)), int(max(samples)) + 1)\n",
    "yy = [binomial_pmf(x, n, p) for x in xx]\n",
    "ax.stem(xx, yy, \"r\", label = r\"Probability mass function\")\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Value')\n",
    "ax.set_ylabel('Probability')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be154956",
   "metadata": {},
   "source": [
    "### 1.4 Exponential Distribution (0.3 points)\n",
    "\n",
    "Construct and test an Exponential random variable generator, by implementing the following functions:\n",
    "* ```exp_rv(lambd)``` - returns a random sample from an Exponential distribution with parameter ```lambd``` (using only ```uniform_rv()```).\n",
    "* ```exp_rv_n(n_samples, lambd)``` - returns a list of ```n_samples``` that are drawn from an Exponential distribution with parameter ```lambd``` (using only ```exp_rv(lambd)```).\n",
    "* ```exp_pdf(x, lambd)``` - returns the probability for a certain ```x``` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f50ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_rv(lambd):\n",
    "    \"\"\"\n",
    "    Generate a single exponentially-distributed random variable.\n",
    "\n",
    "    Parameters:\n",
    "    - lambd (float): the rate parameter.\n",
    "\n",
    "    Returns:\n",
    "    - float: the random variable.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return -np.log(uniform_rv()) / lambd\n",
    "\n",
    "def exp_rv_n(n_samples, lambd):\n",
    "    \"\"\"\n",
    "    Generate n_samples exponentially-distributed random variables.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): the number of samples.\n",
    "    - lambd (float): the rate parameter.\n",
    "\n",
    "    Returns:\n",
    "    - list of floats: the random variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [exp_rv(lambd) for _ in range(n_samples)]\n",
    "\n",
    "def exp_pdf(x, lambd):\n",
    "    \"\"\"\n",
    "    Calculate the probability density function (PDF) of an exponential distribution at a given point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x (float): The point at which to calculate the PDF.\n",
    "    - lambd (float): The rate parameter.\n",
    "\n",
    "    Returns:\n",
    "    - float: The PDF value for the given parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return lambd * np.exp(-lambd * x) if x >= 0 else 0\n",
    "\n",
    "# Run Experiment\n",
    "lambd = 0.7\n",
    "n_samples = 10000\n",
    "samples = exp_rv_n(n_samples, lambd) # Generate exponentially-distributed random variables\n",
    "n_bins = 50\n",
    "\n",
    "# Generate histogram data \n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density = True) # with density scaling\n",
    "\n",
    "# Plotting the histogram as a probability distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1])\n",
    "ax.bar(bin_centers, hist, width = np.diff(bin_centers)[0], edgecolor = 'black', label = \"Data\")\n",
    "\n",
    "# Plotting the actual probability density function\n",
    "xx = np.linspace(0, 10, 10000)\n",
    "yy = [exp_pdf(x, lambd) for x in xx]\n",
    "ax.plot(xx, yy, color = \"r\", label = \"Probability density function$ \")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de5163f5",
   "metadata": {},
   "source": [
    "### 1.5 Poisson Distribution (0.3 points)\n",
    "\n",
    "Construct and test a Poisson random variable generator, by implementing the following functions:\n",
    "* ```poisson_rv(lambd)``` - returns a random sample from a Poisson distribution with parameter ```lambda``` (using only ```exp_rv(lambd)```).\n",
    "* ```poisson_rv_n(n_samples, lambd)``` - returns a list of ```n_samples``` that are drawn from a Poisson distribution with parameter ```lambd``` (using only ```poisson_rv(lambd)```).\n",
    "* ```poisson_pmf(x, lambd)``` - returns the probability for a certain ```x``` value (using only ```np.math.factorial(x)```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_rv(lambd):\n",
    "    \"\"\"\n",
    "    Generate a single Poisson-distributed random variable.\n",
    "\n",
    "    Parameters:\n",
    "    - lambd (float): the rate parameter.\n",
    "\n",
    "    Returns:\n",
    "    - int: the random variable.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    cum, k = 0, -1\n",
    "    while cum < 1:\n",
    "        k += 1\n",
    "        cum += exp_rv(lambd)\n",
    "    return k\n",
    "\n",
    "def poisson_rv_n(n_samples, lambd):\n",
    "    \"\"\"\n",
    "    Generate n_samples Poisson-distributed random variables.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): the number of samples.\n",
    "    - lambd (float): the rate parameter.\n",
    "\n",
    "    Returns:\n",
    "    - list of ints: the random variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [poisson_rv(lambd) for _ in range(n_samples)]\n",
    "    \n",
    "\n",
    "def poisson_pmf(x, lambd):\n",
    "    \"\"\"\n",
    "    Calculate the probability mass function (PMF) of a Poisson distribution at a given point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x (int): The point at which to calculate the PMF.\n",
    "    - lambd (float): The rate parameter.\n",
    "\n",
    "    Returns:\n",
    "    - float: The PMF value for the given parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return (lambd ** x) * np.exp(-lambd) / np.math.factorial(x)\n",
    "\n",
    "# Run Experiment\n",
    "n_samples = 100000\n",
    "lambd = 1\n",
    "samples = poisson_rv_n(n_samples, lambd) # Generate Poisson-distributed random variables\n",
    "n_bins = 100\n",
    "\n",
    "# Generate histogram data \n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density = False) # without density scaling\n",
    "\n",
    "# Plotting the histogram as a probability distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1])\n",
    "ax.bar(bin_centers, np.array(hist) / n_samples, width = np.diff(bin_centers)[5], edgecolor = 'black', label = \"Data\")\n",
    "\n",
    "# Plotting the actual probability density function\n",
    "xx = np.arange(min(samples), max(samples))\n",
    "yy = [poisson_pmf(x, lambd) for x in xx]\n",
    "ax.stem(xx, yy, \"r\", label = \"Probability mass function\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c00f8333",
   "metadata": {},
   "source": [
    "###  1.6. Normal Distribution (0.5 points)\n",
    "\n",
    "Construct and test a Normal random variable generator, by implementing the following functions:\n",
    "* ```normal_rv(mean, var)``` - returns a random sample from a Normal distribution with parameters ```mean``` and ```var``` by simulating random variables in pairs with transition to polar coordinates(using only ```uniform_rv()```).\n",
    "* ```normal_rv_n(n_samples, mean, var)``` - returns a list of ```n_samples``` that are drawn from a Normal distribution with parameters ```mean``` and ```var``` (using only ```normal_rv(mean, var)```).\n",
    "* ```normal_pdf(x, mean, var)``` - returns the probability for a certain ```x``` value.\n",
    "\n",
    "\n",
    "**Hint**: Use the Box-Muller transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77142d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_rv(mean, var):\n",
    "    \"\"\"\n",
    "    Generate a single normally-distributed random variable using polar coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - mean (float): the mean of the distribution.\n",
    "    - var (float): the variance of the distribution.\n",
    "\n",
    "    Returns:\n",
    "    - float: the random variable.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    r = np.sqrt(-2 * np.log(uniform_rv()))\n",
    "    phi = uniform_rv(0, 2 * np.pi)\n",
    "    z0 = r * np.cos(phi)\n",
    "    return z0 * np.sqrt(var) + mean\n",
    "\n",
    "def normal_rv_n(n_samples,mean,var):\n",
    "    \"\"\"\n",
    "    Generate n_samples normally-distributed random variables using polar coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): the number of samples.\n",
    "    - mean (float): the mean of the distribution.\n",
    "    - var (float): the variance of the distribution.\n",
    "\n",
    "    Returns:\n",
    "    - list of floats: the random variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [normal_rv(mean, var) for _ in range(n_samples)]\n",
    "    \n",
    "def normal_pdf(x, mean, var):\n",
    "    \"\"\"\n",
    "    Calculate the probability density function (PDF) of a normal distribution at a given point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x (float): The point at which to calculate the PDF.\n",
    "    - mean (float): The mean of the distribution.\n",
    "    - var (float): The variance of the distribution.\n",
    "\n",
    "    Returns:\n",
    "    - float: The PDF value for the given parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return 1 / np.sqrt(2 * np.pi * var) * np.exp((x - mean) ** 2 / -(2 * var))\n",
    "\n",
    "# Run Experiment\n",
    "n_samples = 100000\n",
    "mean = 10\n",
    "var = 3\n",
    "samples = normal_rv_n(n_samples, mean, var) # Generate normally-distributed random variables\n",
    "n_bins = 50\n",
    "\n",
    "# Generate histogram data \n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density=True) # with density scaling\n",
    "\n",
    "# Plotting the histogram as a probability distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1])\n",
    "ax.bar(bin_centers, hist, width = np.diff(bin_centers)[0], edgecolor = 'black', label = \"Data\")\n",
    "\n",
    "# Plotting the actual probability density function\n",
    "xx = np.linspace(min(samples), max(samples), 10000)\n",
    "yy = [normal_pdf(x, mean, var) for x in xx]\n",
    "ax.plot(xx, yy, color = \"r\", label = \"Probability density function\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f6fe633",
   "metadata": {},
   "source": [
    "### 1.7 Chi-squared Distribution (0.5 points)\n",
    "\n",
    "Construct and test a $\\chi^2$ random variable generator, by implementing the following functions:\n",
    "* ```chi_squared_rv(k)``` - returns a random sample from a $\\chi^2$ distribution with `k` degrees of freedom (using only ```normal_rv_n()```).\n",
    "* ```chi_squared_rv_n(n_samples, k)``` - returns a list of `n_samples` that are drawn from a $\\chi^2$ distribution with `k` degrees of freedom (using only ```chi_squared_rv(k)```).\n",
    "* ```chi_squared_pdf(x, k)``` - returns the probability for a certain `x` value (you are allowed to use ```scipy.stats.gamma()``` in your implementation).\n",
    "\n",
    "**Hint:** Chi-distribution can be expressed as a sum of normally distributed variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "\n",
    "def chi_squared_rv(k):\n",
    "    \"\"\"\n",
    "    Generate a single random variable from a chi squared distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - k (int): the number of degrees of freedom.\n",
    "\n",
    "    Returns:\n",
    "    - float: the random variable.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    normals = normal_rv_n(k, 0, 1)  # Generate k standard normal random vars\n",
    "    chi_squared = sum([x**2 for x in normals])\n",
    "    return chi_squared\n",
    "\n",
    "def chi_squared_rv_n(n_samples, k):\n",
    "    \"\"\"\n",
    "    Generate n_samples random variables from a chi squared distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): the number of samples.\n",
    "    - k (int): the number of degrees of freedom.\n",
    "\n",
    "    Returns:\n",
    "    - list of floats: the random variables.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return [chi_squared_rv(k) for _ in range(n_samples)]\n",
    "\n",
    "\n",
    "def chi_squared_pdf(x, k):\n",
    "    \"\"\"\n",
    "    Calculate the probability density function (PDF) of a chi squared distribution at a given point x.\n",
    "\n",
    "    Parameters:\n",
    "    - x (float): The point at which to calculate the PDF.\n",
    "    - k (int): The number of degrees of freedom.\n",
    "\n",
    "    Returns:\n",
    "    - float: The PDF value for the given parameters.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return gamma.pdf(x, k/2, scale=2)\n",
    "\n",
    "# Run Experiment\n",
    "n_samples = 25000\n",
    "k = 2\n",
    "samples = chi_squared_rv_n(n_samples, k)\n",
    "n_bins = 50\n",
    "\n",
    "# Generate histogram data \n",
    "hist, bin_edges = custom_histogram(samples, n_bins, density = True) # with density scaling\n",
    "\n",
    "# Plotting the histogram as a probability distribution\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1])\n",
    "ax.bar(bin_centers, hist, width = np.diff(bin_centers)[0], edgecolor = 'black', label = \"Data\")\n",
    "\n",
    "# Plotting the actual probability density function\n",
    "xx = np.linspace(min(samples), max(samples), 10000)\n",
    "yy = [chi_squared_pdf(x, k) for x in xx]\n",
    "ax.plot(xx, yy, color = \"r\", label = \"Probability density function\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d98bda3c",
   "metadata": {},
   "source": [
    "## 2. Experiments (2.5 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c24d906",
   "metadata": {},
   "source": [
    "### 2.1. Coin Flip Experiment (0.5 points)\n",
    "\n",
    "Consider the game of flipping a coin, an endless sequence of independent trials with the flip of the fair coin. Let us define by $X_1, X_2 \\dots X_N$ a sequence of independent identically distributed random variables, each of which takes on values 1 if in the corresponding trial the tail is drawn, and −1 otherwise (with probability = 1/2). Denote the total gain by $S_n = X_1 + X_2 + . . . + X_n$. Perform $N = 1000$ trials and plot multiple realizations of the process $S_n$ at $n = 0, 1, . . . , N$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coinToss():\n",
    "    \"\"\"\n",
    "    Simulate a single coin toss game,.\n",
    "\n",
    "    Returns:\n",
    "    - int: the random variable X_i - 1 if heads, -1 if tails\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return np.random.choice([1, -1])\n",
    "\n",
    "def coinTossGame(n):\n",
    "    \"\"\"\n",
    "    Simulate a coin toss game with n rounds.\n",
    "\n",
    "    Parameters:\n",
    "    - n (int): the number of rounds.\n",
    "\n",
    "    Returns:\n",
    "    - list of ints: [S1, S2, S3, ...] - the cumulative sum of the game at each round.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    Xn = [coinToss() for _ in range(n)]\n",
    "    Sn = np.cumsum(Xn)\n",
    "    return Sn\n",
    "\n",
    "# Run Experiment\n",
    "n = 1000\n",
    "n_experiments = 50\n",
    "fig, ax = plt.subplots(1, 1, figsize = (15, 7))\n",
    "for i in range(n_experiments):\n",
    "    Sn = coinTossGame(n)\n",
    "    ax.plot(np.arange(0, 1, 1 / n), Sn, alpha = 0.5)\n",
    "ax.set_title(r\"Multiple realizations of the $S_n$ process\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7cdcfce9",
   "metadata": {},
   "source": [
    "### 2.1.2. Law of large numbers (0.5 points)\n",
    "\n",
    "Verify empirically the law of large numbers using your coinToss game, i.e. investigate the behavior of the process $X(n) = S_n/n$. What happens as `n` increases? How does this demonstrate the law of large numbers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "686c7e2c",
   "metadata": {},
   "source": [
    "<div style=\"background: #f6e28b; padding: 20px; color: black;\">\n",
    "\n",
    "<b>WRITE YOUR ANSWER TO THE QUESTION ABOVE HERE (DOUBLE-CLICK TO EDIT)</b>\n",
    "\n",
    "<br> To verify the LLN using the coinToss game simulation, we observe the behaviour of the process X(n) = S_n / n. As n increases, the value of X(n) tends to converge towards the xpected value of a single coin toss. In the case of a fair coin, the expected value is 0, since the probability of getting heads (1) is equal to tails (-1), making their average 0. The LLN states that as the number of trials increases, the average of the outcomes will get closer to the expected value. With a large number of tosses, the effects of randomness are averaged out, and the proportion of heads to tails should approach 1:1, reflecting the fair nature of the coin. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cfa3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25000  # Increase n for a clearer demonstration of LLN\n",
    "\n",
    "# YOUR CODE HERE\n",
    "def coinTossGameWithAverage(n):\n",
    "    Sn = coinTossGame(n)\n",
    "    average = Sn / np.arange(1, n + 1)\n",
    "    return average\n",
    "\n",
    "# Generate the cumulative average\n",
    "x = coinTossGameWithAverage(n)\n",
    "\n",
    "# Plot the cumulative average against the number of tosses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(1, n + 1), x, label = 'Cumulative Average of Outcomes')\n",
    "plt.axhline(y = 0, color = 'r', linestyle = '-', label = 'Expected Value (0)')\n",
    "plt.xlabel('Number of Tosses')\n",
    "plt.ylabel('Cumulative Average')\n",
    "plt.title('Law of Large Numbers Demonstration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0addc9cb",
   "metadata": {},
   "source": [
    "### 2.1.3. Central Limit Theorem (0.5 points)\n",
    "Verify empirically the central limit theorem, by running the game multiple times. For each game, save the total sum $S_n$ and plot the frequency of the values using the histogram function. What distribution does the histogram follow? How to scale this histogram to represent the standard distribution? How does this demonstrate the Central Limit Theorem?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b1da0a0",
   "metadata": {},
   "source": [
    "<div style=\"background: #f6e28b; padding: 20px; color: black;\">\n",
    "\n",
    "<b>WRITE YOUR ANSWER TO THE QUESTION ABOVE HERE (DOUBLE-CLICK TO EDIT)</b>\n",
    "\n",
    "<br> By running the coin toss game multiple times and plotting the frequency of the total sums, S_n, the histogram follows a normal distribution. This empirical result verifies the Central Limit Theorem, which indicates that the sum of a large number of independent and identically distributed random variables, regardless of their distribution, will approximate a normal distribution. To scale this histogram to represent the standard normal distribution, you would standardise the sums by subtracting the mean and dividing by the standard deviation. This process illustrates the CLT by showing how a large sample size leads to a distribution of sums that is normally distributed, even from a simple binary outcome like a coin toss. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc5486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "for (n_games, n) in [(1000, 1000),(2000,2000)]: # Second run takes ~25 sec\n",
    "    \n",
    "    sums = [coinTossGame(n)[-1] for _ in range(n_games)] # Last element -> total sum\n",
    "\n",
    "    # Calculate the actual mean and variance of the sums\n",
    "    actual_mean = np.mean(sums)\n",
    "    actual_var = np.var(sums)\n",
    "\n",
    "    # Generate histogram data \n",
    "    hist, bin_edges = custom_histogram(sums, 50, density=True) # with density scaling\n",
    "\n",
    "    # Plotting the histogram as a probability distribution\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (9, 5))\n",
    "    bin_centers = (np.diff(bin_edges) / 2 + bin_edges[:-1])\n",
    "    ax.bar(bin_centers, hist, width = np.diff(bin_centers)[0], edgecolor = 'black', label = \"Data\")\n",
    "\n",
    "    # Plotting the actual probability density function\n",
    "    xx = np.linspace(min(sums), max(sums), 1000)\n",
    "    yy = [normal_pdf(x, actual_mean, actual_var) for x in xx]\n",
    "    plt.plot(xx, yy, color = \"r\", label = f\"Normal probability density function with $\\\\mu = {actual_mean:0.2f}$ and $\\\\sigma^2 = {actual_var:0.2f}$\")\n",
    "    plt.title('Central Limit Theorem Demonstration')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26679b94",
   "metadata": {},
   "source": [
    "### 2.2 Unit Simplex (1 point)\n",
    "Sample from $X =\\{(x_1, x_2, x_3)|0 ≤ x_i ≤ 1, x_1 + x_2 + x_3 = 1\\}$ in a way that most of the samples are:\n",
    "* concentrated in the center (0.25 points)\n",
    "* uniformly distributed (0.25 points)\n",
    "\n",
    "We will visualize the points in two dimensions. You can use the approach from the lectures, or try to come up with your own. You are allowed to use any external libraries for this experiment. Explain the solution you have implemented.\n",
    "\n",
    "**Hint:** The distribution on a unit simplex is also known as the Dirichlet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet\n",
    "\n",
    "n_samples = 10000\n",
    "\n",
    "samples_centered = np.zeros((n_samples, 3)) # Replace this with your concentrated samples\n",
    "samples_uniform = np.zeros((n_samples, 3)) # Replace this with your uniform samples\n",
    "\n",
    "# YOUR CODE HERE\n",
    "alpha_centered = [10, 10, 10]  \n",
    "samples_centered = dirichlet(alpha_centered).rvs(n_samples)\n",
    "# Sampling uniformly distributed\n",
    "alpha_uniform = [1, 1, 1]  \n",
    "samples_uniform = dirichlet(alpha_uniform).rvs(n_samples)\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(1, 2, figsize = (14, 7))\n",
    "\n",
    "# Plot for concentrated samples\n",
    "ax[0].scatter(samples_centered[:, 0], samples_centered[:, 1], alpha = 0.5)\n",
    "ax[0].set_title('Concentrated Samples')\n",
    "ax[0].set_xlabel('$x_1$')\n",
    "ax[0].set_ylabel('$x_2$')\n",
    "\n",
    "# Plot for uniformly distributed samples\n",
    "ax[1].scatter(samples_uniform[:, 0], samples_uniform[:, 1], alpha = 0.5)\n",
    "ax[1].set_title('Uniformly Distributed Samples')\n",
    "ax[1].set_xlabel('$x_1$')\n",
    "ax[1].set_ylabel('$x_2$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66b84549",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo (4.5 points)\n",
    "\n",
    "Monte Carlo (MC) methods are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. Their essential idea is using randomness to solve problems that might be deterministic in principle. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. MC methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6d389b4",
   "metadata": {},
   "source": [
    "### 3.0 Estimating $\\pi$ using Monte Carlo (0.5 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f667274c",
   "metadata": {},
   "source": [
    "We can approximate $\\pi$, by drawing samples uniformly from inside a square and then counting how many of these points fall within the inscribed circle.\n",
    "\n",
    "The circle is inscribed inside the square, which means that $l = 2r,$ where $r$ is radius of the circle. The area of the circle is given by $A_{circle} = \\pi * r^2$ and the area of the square is given by $A_{square} = l ^ 2$. Now, we calculate the ratio of the two areas:\n",
    "$$ \\frac{A_{circle}}{A_{square}} = \\frac{\\pi * r^2}{l^2} = \\frac{\\pi * r^2}{(2r)^2} = \\frac{\\pi * r^2}{4r^2} = \\frac{\\pi}{4}$$\n",
    "\n",
    "By rewriting the formula we can obtain the formula for $\\pi$ in terms of the ratio:\n",
    "\n",
    "$$ \\pi = 4 * \\frac{A_{circle}}{A_{square}} $$\n",
    "\n",
    "By drawing samples uniformly, we can approximate this ratio by dividing the number of points that fall within the circle by the total number of points: \n",
    "$$\\frac{A_{circle}}{A_{square}} \\approx \\frac{N_{in}}{N} $$ \n",
    "\n",
    "By replacing the ratio with our approximation in the formula for $\\pi$, we will get the final approximation:\n",
    "\n",
    "$$\\pi \\approx 4 * \\frac{N_{in}}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914fb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can just run this cell\n",
    "def draw_circle_inside_square(ax, r, inside_points = None, outside_points = None):\n",
    "    circle = plt.Circle((0, 0), r, color='r', fill = False)\n",
    "    square = plt.Rectangle((-r, -r), 2 * r, 2 * r, fill = False, color = 'b', linewidth = 2)\n",
    "    ax.add_patch(circle)\n",
    "    ax.add_patch(square)\n",
    "    ax.set_xlim([-r, r])\n",
    "    ax.set_ylim([-r, r])\n",
    "    if inside_points:\n",
    "        ax.scatter([x[0] for x in inside_points], [x[1] for x in inside_points], color = 'r', linewidths = 1, label = f\"{len(inside_points)} inside points\")\n",
    "    if outside_points:\n",
    "        ax.scatter([x[0] for x in outside_points], [x[1] for x in outside_points], color = 'b', linewidths = 1, label = f\"{len(outside_points)} outside points\")\n",
    "    ax.legend()\n",
    "fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n",
    "draw_circle_inside_square(ax, 5, inside_points = [(0, 0)], outside_points = [(-4, 4)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11848721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside_circle(x, y, r):\n",
    "    \"\"\"\n",
    "    Check if a point is inside a circle of radius r.\n",
    "\n",
    "    Parameters:\n",
    "    - x (float): x-coordinate of the point.\n",
    "    - y (float): y-coordinate of the point.\n",
    "    - r (float): radius of the circle.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the point is inside the circle, False otherwise.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return x**2 + y**2 <= r**2\n",
    "\n",
    "def calculate_approximation(N_in, N):\n",
    "    \"\"\"\n",
    "    Calculate the approximation of pi using the ratio of points inside a circle to the total points.\n",
    "\n",
    "    Parameters:\n",
    "    - N_in (int): the number of points inside the circle.\n",
    "    - N (int): the total number of points.\n",
    "\n",
    "    Returns:\n",
    "    - float: the approximation of pi.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return (N_in / N) * 4\n",
    "\n",
    "def calculate_pi(N, r):\n",
    "    \"\"\"\n",
    "    Calculate the approximation of pi using the Monte Carlo method.\n",
    "\n",
    "    Parameters:\n",
    "    - N (int): the number of points to generate.\n",
    "    - r (float): the radius of the circle.\n",
    "\n",
    "    Returns:\n",
    "    - list of floats: the approximations of pi at each iteration.\n",
    "    - list of tuples: the generated points.\n",
    "    \"\"\"\n",
    "    N_in = 0\n",
    "    approximations, inside_points, outside_points = [], [], []\n",
    "    # YOUR CODE HERE\n",
    "    for _ in range(N):\n",
    "        x, y = uniform_rv(-r, r), uniform_rv(-r, r)\n",
    "        if inside_circle(x, y, r): inside_points.append((x, y))\n",
    "        else: outside_points.append((x, y))\n",
    "        if inside_circle(x, y, r):\n",
    "            N_in += 1\n",
    "        approximations.append(calculate_approximation(N_in, _ + 1))\n",
    "    return approximations, inside_points, outside_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3da48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3000\n",
    "radius = 50\n",
    "approximations, inside_points, outside_points = calculate_pi(N, radius)\n",
    "print(f\"Last approximation: {approximations[-1]}\")\n",
    "\n",
    "# Draw the circle\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "draw_circle_inside_square(ax[0], radius, inside_points, outside_points)\n",
    "\n",
    "# Draw the evolution of our approximation\n",
    "ax[1].plot(approximations, label = \"Approximation\")\n",
    "\n",
    "# Draw the value of 𝜋 as a horizontal red line\n",
    "ax[1].axhline(y = np.pi, color = 'r', linestyle = '-', label = \"Actual Value\") \n",
    "ax[1].title.set_text('Approximation of $\\\\pi$')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68362725",
   "metadata": {},
   "source": [
    "Copy the code from the previous cell and run multiple experiments for different values of N. What happens as the number of samples increases? How many iteration are needed to get a good approximation?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b772faef",
   "metadata": {},
   "source": [
    "<div style=\"background: #f6e28b; padding: 20px; color: black;\">\n",
    "\n",
    "<b>WRITE YOUR ANSWER TO THE QUESTION ABOVE HERE (DOUBLE-CLICK TO EDIT)</b>\n",
    "\n",
    "<br> The value of N depends on the definition of 'a good approximation'. In the 3 experiments we used the values 10, 1,000 and 1,000,000. For the value of 10, the approximation is definitely not clear enough, because running this cel multiple times generates a value between about 2 and 4. If we increase N to be 1,000, we already gain a more reasonable answer for pi, ranging from 3.0 to 3.2. But this estimation is not good enough yet. Once we increase the value of N to about 1,000,000 the results range between 3.141 and 3.142 which already indicates the first 3 digits of pi. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c937dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 50\n",
    "for N in [10, 1000, 1000000]:\n",
    "    approximations, inside_points, outside_points = calculate_pi(N, radius)\n",
    "    print(f\"Last approximation: {approximations[-1]}\")\n",
    "\n",
    "    # Draw the circle\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (15, 7))\n",
    "    draw_circle_inside_square(ax[0], radius, inside_points, outside_points)\n",
    "\n",
    "    # Draw the evolution of our approximation\n",
    "    ax[1].plot(approximations, label = \"Approximation\")\n",
    "\n",
    "    # Draw the value of 𝜋 as a horizontal red line\n",
    "    ax[1].axhline(y = np.pi, color = 'r', linestyle = '-', label = \"Actual Value\") \n",
    "    ax[1].title.set_text('Approximation of $\\\\pi$')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "487bae57",
   "metadata": {},
   "source": [
    "### 3.1 Importance Sampling\n",
    "\n",
    "Importance sampling is a MC technique used for calculating the integral of a function by generating samples from a probability distribution that closely resembles the target function, often referred to as the posterior. This approach is useful when the function of interest is complex and can not be solved analytically.\n",
    "\n",
    "Let's say we want to evaluate an integral $I = \\int_a^b f(x) \\,dx$, wich due to the nature of $f(x)$ can not be calculated analytically. We are going to approximate it with the expectation over a probability distribution $q(x)$.\n",
    "\n",
    "$$ I = \\int_a^b f(x) \\,dx = \\int_a^b f(x) \\frac{q(x)}{q(x)} \\,dx = \\int_a^b \\frac{f(x)}{q(x)} q(x) \\,dx = \\mathbb{E}_{q(x)} \\left[ \\frac{f(x)}{q(x)} \\right] \\approx \\frac{1}{N}\\sum_{i=1}^{N} \\frac{f(x_i)}{q(x_i)}$$\n",
    "\n",
    "Here's an outline of the steps involved in coding this approach:\n",
    "1. **Define the target function $f(x)$**: This is the function inside the integral.\n",
    "2. **Choose an importance distribution $q(x)$**: This should be a distribution over the same domain as the integral.\n",
    "3. **Generate samples from $q(x)$**: Use random sampling methods to draw samples from $q(x)$.\n",
    "4. **Calculate weights**: For each sample $x_i$, calculate the weight $w_i = \\frac{f(x_i)}{q(x_i)}$.\n",
    "5. **Estimate the integral**: The integral is approximated as the average of the weights $ \\hat{I}  = \\frac{1}{N}\\sum_{i=1}^{N} w_i$.\n",
    "\n",
    "**Note:** Most of the functions in this assignment are simple and can be solved directly. They are used to explain the concept and to check the results against known values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78980dc9",
   "metadata": {},
   "source": [
    "### 3.1.1 Simple Case (0.5 points)\n",
    "\n",
    "The simplest sampling technique is to draw the $x_i$ values from an uniform distribution $x_i \\sim Uni(a,b) $, i.e. $q(x_i) = \\frac{1}{b-a}$. Plugging this into the general formula gives us the following approximation:\n",
    "\n",
    "$$ \\hat{I} = \\frac{1}{N}\\sum_{i=1}^{N} \\frac{f(x_i)}{\\frac{1}{(a-b)}} = \\frac{1}{N}\\sum_{i=1}^{N} f(x_i)(a-b)$$\n",
    "\n",
    "You can interpret this as taking the value $f(x)$ and scaling it by $b - a$. This can be visualised as taking multiple rectangles with height $H = f(x_i)$ and width $W = b - a$ at multiple locations, given by our sampler, and approxmiating the final area by the average of the rectangles' areas. You can check this visually by running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interval [a, b] and sample points\n",
    "a, b = 1, 3\n",
    "x_samples = np.array([1, 2, 2.8])\n",
    "y_samples = x_samples ** 2\n",
    "\n",
    "for i, (x_i, y_i) in enumerate(zip(x_samples, y_samples), start = 1):\n",
    "    plt.figure(figsize = (8, 5))\n",
    "\n",
    "    # Plot the function\n",
    "    x = np.linspace(a - 1, b + 1, 1000)\n",
    "    y = x ** 2\n",
    "    plt.plot(x, y, 'r-', label = 'f(x) = $x^2$')\n",
    "\n",
    "    # Plot rectangle for the current sample point\n",
    "    rect = plt.Rectangle((a, 0), b - a, y_i, color = 'green', alpha = 0.2, label = 'Area given by $f(x) * (b - a)$')\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "    # Highlight the current sample point\n",
    "    plt.scatter(x_i, y_i, color = 'blue')\n",
    "\n",
    "    # Highlight the interval [a, b]\n",
    "    plt.axvline(x = a, color = 'black', linestyle = '--', label = 'a')\n",
    "    plt.axvline(x = b, color = 'black', linestyle = '--', label = 'b')\n",
    "\n",
    "    plt.title(f'Visualization for Sample Point {i}: f(x) = $x^2$')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5998f12",
   "metadata": {},
   "source": [
    "You are given the function $f_1(x) = x^2$. You are tasked with evaluating the integral $\\int_a^b f_1(x) \\,dx$ using importance sampling with the uniform distribution $q(x) = Uni(a,b)$ as you importance distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x):\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "def simple_mc(N, f, a, b):\n",
    "    \"\"\"\n",
    "    Estimate the integral of f over the interval [a, b] using simple Monte Carlo method (uniform sampling).\n",
    "\n",
    "    Parameters:\n",
    "    - N (int): the number of samples.\n",
    "    - f (function): the function to integrate.\n",
    "    - a (float): the start of the interval.\n",
    "    - b (float): the end of the interval.\n",
    "\n",
    "    Returns:\n",
    "    - float: the estimated value of the integral.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    samples = np.array(uniform_rv_n(N, a, b))  # Generate N samples uniformly distributed between a and b\n",
    "    f_samples = f(samples)  # Evaluate f at each sample point\n",
    "    integral_estimate = (b - a) * np.mean(f_samples)  # Calculate the average of the function values and scale by the interval length\n",
    "    return integral_estimate\n",
    "\n",
    "# Run test\n",
    "a = 1\n",
    "b = 2\n",
    "N = 100000\n",
    "xx = np.linspace(a - 1, b + 1, 10000)\n",
    "yy = f1(xx)\n",
    "plt.plot(xx, yy, label = \"The function $f(x) = x^2$\")\n",
    "plt.fill_between(x = xx, y1 = f1(xx), \n",
    "        where = (a < xx) & (xx < b),\n",
    "        color = \"b\",\n",
    "        alpha = 0.2,\n",
    "        label = \"Area under the curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "approximation = simple_mc(N, f1, a, b)\n",
    "print(f\"Your approximation : {approximation}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca8992b5",
   "metadata": {},
   "source": [
    "It it easy to check the answer analytically by solving the integral $\\int_a^b x^2 \\,dx =\\left[\\frac{x^3}{3}\\right]_a^b = \\frac{b^3 - a^3}{3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ce902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_integral(a, b):\n",
    "    return (b ** 3 - a ** 3) / 3\n",
    "\n",
    "# Run this cell to test your solution\n",
    "def run_test(N, a, b):\n",
    "    approx = simple_mc(N, f1, a, b)\n",
    "    result = f1_integral(a, b)\n",
    "    error = np.abs(result - approx)\n",
    "    print(f\"integral on [{a},{b}] as estimated from {N} Samples: {approx:.3f}; actual answer: {result:.3f}; error :{error:.3f}\")\n",
    "    return error\n",
    "\n",
    "assert run_test(1000, 1, 2) < 0.1 \n",
    "assert run_test(5000, -2, 1) < 0.1 \n",
    "assert run_test(10000, -2, 2) < 0.5\n",
    "assert run_test(10000, -5, 5) < 5\n",
    "assert run_test(100000, 0, 15) < 10\n",
    "assert run_test(1000000, -10, 10) < 5\n",
    "assert run_test(10000000, 0, 50) < 25"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b5303cc",
   "metadata": {},
   "source": [
    "### 3.1.2 General Case  (0.5 points)\n",
    "\n",
    "We will consider a more complex function $f_2(x) = a * \\exp \\{ -\\frac{1}{2 * \\sigma ^ 2}(x - \\mu)^2 \\}$ which looks like a normal distribution. \n",
    "Note that due to the scaling factor $a$ this is not always a probability density function (only for $a = \\frac{1}{\\sigma * \\sqrt{2 * \\pi}}$).\n",
    "Use importance sampling to evaluate the integral $\\int_{-\\infty}^{\\infty} f_2(x) \\,dx$.\n",
    "\n",
    "**Hint:** Use the normal distribution as your importance distribution $q(x) = \\mathcal{N}(\\mu, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf0e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x, a, mu, sigma):\n",
    "    return a * np.exp(-(x - mu) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "def importance_mc(N, a, mu, sigma):\n",
    "    \"\"\"\n",
    "    Estimate the integral of f2 using importance sampling with q(x) being the normal distribution N(mu, sigma^2).\n",
    "    \n",
    "    Parameters:\n",
    "    - N (int): the number of samples.\n",
    "    - a (float): the scale of the function.\n",
    "    - mu (float): the mean of the normal distribution.\n",
    "    - sigma (float): the standard deviation of the normal distribution.\n",
    "\n",
    "    Returns:\n",
    "    - float: the estimated value of the integral.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    samples = np.array(normal_rv_n(N, mu, sigma ** 2))\n",
    "    weights = f2(samples, a, mu, sigma) / normal_pdf(samples, mu, sigma ** 2)\n",
    "    integral_estimate = np.mean(weights)\n",
    "    return integral_estimate\n",
    "\n",
    "# Run your function\n",
    "N = 10000\n",
    "a = 2\n",
    "mu = 1\n",
    "sigma = 3\n",
    "approximation = importance_mc(N, a, mu, sigma)\n",
    "print(f\"Your approximation : {approximation}\")\n",
    "\n",
    "# Plot the function and the area under the curve\n",
    "x_min = mu - 3 * sigma\n",
    "x_max = mu + 3 * sigma\n",
    "xx = np.linspace(x_min, x_max, 1000) # generate array of x\n",
    "yy = [f2(x, a, mu, sigma) for x in xx] # calculate the f2(x) for every x\n",
    "plt.plot(xx,yy, label = \"The function\")\n",
    "plt.fill_between(x = xx, y1 = yy, where = (x_min < xx) & (xx < x_max), alpha = 0.2, label = \"Area under the curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "604d5fe6",
   "metadata": {},
   "source": [
    "We can again compute the answer analytically by solving the integral $\\int_{-\\infty}^{\\infty} f(x) \\,dx = a \\int_{-\\infty}^{\\infty} \\exp \\{ -\\frac{1}{2 * \\sigma ^ 2}(x - \\mu)^2 \\} \\,dx = a * \\sqrt{2 * \\pi} * \\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f7feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to test your solution\n",
    "def actual_f2(a, mu, sigma):\n",
    "    return a * np.sqrt(2 * np.pi) * sigma\n",
    "\n",
    "def run_test2(N, a, mu, sigma):\n",
    "    approx = importance_mc(N, a, mu, sigma)\n",
    "    answer = actual_f2(a, mu, sigma)\n",
    "    error = np.abs(answer - approx)\n",
    "    print(f\"integral as estimated from {N} Samples: {approx:.5f}; actual answer: {answer:.5f}; error :{error}\")\n",
    "    return error\n",
    "\n",
    "assert run_test2(1000, 2, 1, 3) < 1e-10\n",
    "assert run_test2(1000, 5, 5, 2) < 1e-10\n",
    "assert run_test2(1000, 5, 10, 4) < 1e-10\n",
    "assert run_test2(1000, 10, 10, 10) < 1e-10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d90fa3c",
   "metadata": {},
   "source": [
    "### 3.1.3 Multidimensional Integral (1 point)\n",
    "\n",
    "Use importance sampling to approximate the following integral by using the exponential distribution as your importance distribution $q(x)$ (you are allowed use external libraries in your implementation):\n",
    "\n",
    "$$ I = \\int_{0}^{\\infty} \\ldots \\int_{0}^{\\infty} \\exp\\left( -x_1 - \\ldots - x_{10} - \\frac{1}{2^{20} x_1 \\ldots x_{10}} \\right) x_{1}^{\\frac{1}{11} - 1} x_{2}^{\\frac{2}{11} - 1} \\dots x_{10}^{\\frac{10}{11} - 1}  dx_1 \\ldots dx_{10} $$\n",
    "\n",
    "**Hint:** The joint pdf of 10 independent exponential random variables $X_1, X_2, \\ldots, X_{10}$, each having the same parameter $\\lambda$, is the product of the individual pdfs $ f(x_1, x_2, \\ldots, x_{10}) = \\lambda e^{-\\lambda x_1} \\cdot \\lambda e^{-\\lambda x_2} \\cdot \\ldots \\cdot \\lambda e^{-\\lambda x_{10}} = \\lambda^{10} e^{-\\lambda (x_1 + x_2 + \\ldots + x_{10})} \\quad \\text{for each } x_i \\geq 0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aff888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function made by us\n",
    "def multidimensional_integral(N, lambda_param, dim):\n",
    "    # Sample variables for approximation\n",
    "    samples = np.random.exponential(1 / lambda_param, (N, dim)) # Using numpy's because it's more convenient for 2D samples and also much quicker\n",
    "    # samples = []\n",
    "    # for _ in range(dim):\n",
    "    #     samples.append(exp_rv_n(N, 1 / lambda_param))\n",
    "    prod_samples = np.prod(samples, axis=1)  # Product of x_i for each sample\n",
    "    sum_samples = np.sum(samples, axis=1) # Sum of x_i for each sample\n",
    "    powers = np.prod(samples ** (np.arange(1, (dim + 1)) / (dim + 1) - 1), axis=1) # Product of x_i^(i/11 - 1)\n",
    "\n",
    "    # Calculate the weights\n",
    "    f_x = np.exp(-sum_samples - 1/(2 ** (2 * dim) * prod_samples)) * powers\n",
    "    q_x = (lambda_param ** dim) * np.exp(-lambda_param * sum_samples)\n",
    "    \n",
    "    # Importance sampling estimate\n",
    "    weights = f_x / q_x\n",
    "    integral_estimate = np.mean(weights)\n",
    "    \n",
    "    return integral_estimate\n",
    "\n",
    "# Define the number of samples and lambda parameter for the exponential distribution\n",
    "N = 10_000_000\n",
    "lambda_param = 1  # Assume lambda = 1 for simplicity\n",
    "dim = 10 # Generalize dimension for verification on lower dimensions\n",
    "\n",
    "# Compute the approximation of the multidimensional integral\n",
    "I_hat = multidimensional_integral(N, lambda_param, dim)\n",
    "print(f\"Approximation: I = {I_hat}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "000ce64e",
   "metadata": {},
   "source": [
    "### 3.2 Introduction to the Metropolis-Hastings Algorithm\n",
    "\n",
    "The Metropolis-Hastings (M–H) algorithm is a simple yet powerful approach for Markov Chain Monte Carlo (MCMC) sampling, designed to sample from a probability distribution when direct sampling is difficult. This is particularly useful in Bayesian inference, where the goal is to sample from the posterior distribution of parameters given some observed data. The algorithm can be broken down into several key components and steps:\n",
    "\n",
    "1. **Initialization**: Start with an initial parameter value $q_0$.\n",
    "\n",
    "2. **Proposal Distribution**: At each step, generate a new candidate parameter value $q'$ from a proposal distribution $Q(q'|q)$, which depends on the current parameter value $q$. This proposal distribution must be symmetric but can take various forms, such as a Gaussian distribution centered at the current parameter value.\n",
    "\n",
    "3. **Acceptance Probability**: The algorithm then calculates the acceptance probability $A(q'|q)$, which is determined by the ratio of the target distributions evaluated at the new and current parameter values, as well as the proposal distribution for the proposed and current states. For the Metropolis-Hastings algorithm, the acceptance probability is given by:\n",
    "   $$\n",
    "   A(q'|q) = \\min\\left(1, \\frac{P(q')Q(q|q')}{P(q)Q(q'|q)}\\right)\n",
    "   $$\n",
    "   Here, $P(q)$ is the target distribution from which we wish to sample (e.g., the posterior distribution in Bayesian inference), and $Q(q'|q)$ is the proposal distribution. In the Metropolis algorithm, a special case of Metropolis-Hastings, the proposal distribution is symmetric, simplifying the acceptance probability to:\n",
    "   $$\n",
    "   A(q'|q) = \\min\\left(1, \\frac{P(q')}{P(q)}\\right)\n",
    "   $$\n",
    "\n",
    "4. **Sampling Step**: A new sample value $q'$ is accepted with the probability $A(q'|q)$. If $q'$ is rejected, the algorithm retains the current value $q$ as the next sample. This decision is made by drawing a uniform random number $u$ in the range $[0,1]$ and accepting $q'$ if $u < A(q'|q)$.\n",
    "\n",
    "5. **Iteration**: Repeat steps 2-4 for a large number of iterations. After a burn-in period, the sampled values of $q$ approximate the desired target distribution.\n",
    "\n",
    "6. **Convergence and Tuning**: The efficiency of the Metropolis-Hastings algorithm depends on the choice of the proposal distribution. Tuning the parameters of this distribution (e.g., the variance of a Gaussian proposal) is crucial for achieving efficient sampling and ensuring the chain explores the parameter space adequately.\n",
    "\n",
    "The Metropolis-Hastings algorithm is powerful because it allows for sampling from complex, multi-dimensional distributions that are common in statistical inference problems. However, its performance heavily depends on the choice of the proposal distribution and the starting point of the chain. Proper tuning and diagnostics are essential for ensuring that the sampled distribution accurately represents the target distribution. You can read more in this [article by Hogg & Foreman-Mackey](https://iopscience-iop-org.tudelft.idm.oclc.org/article/10.3847/1538-4365/aab76e/meta). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a16330d",
   "metadata": {},
   "source": [
    "### 3.2.1 Sample from a Gaussian Distribution (0.5 points)\n",
    "\n",
    "Please implement the M-H algorithm below and test it on the target distribution $q = \\mathcal{N}(\\mu,\\,\\sigma^{2})$, with $\\mu = 2$ and $\\sigma^{2} = 2$. This is Problem 2 from [Hogg & Foreman-Mackey](https://iopscience-iop-org.tudelft.idm.oclc.org/article/10.3847/1538-4365/aab76e/meta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4195bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x_values):\n",
    "    # Parameters for the normal target distribution\n",
    "    mu = 2 # Mean\n",
    "    var = 2  # Variance\n",
    "    return (1 / np.sqrt(2 * np.pi * var)) * np.exp(-0.5 * ((x_values - mu) ** 2 / var))\n",
    "\n",
    "def metropolis_hastings(n_steps, q):\n",
    "    \"\"\"\n",
    "    Perform Metropolis-Hastings sampling to generate samples from the target distribution q.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_steps (int): the number of MCMC steps.\n",
    "    - q (function): the target distribution.\n",
    "\n",
    "    Returns:\n",
    "    - list of floats: the generated samples.\n",
    "    \"\"\"\n",
    "    samples = [2]\n",
    "    # YOUR CODE HERE\n",
    "    for _ in range(1, n_steps):\n",
    "        current_sample = samples[-1]\n",
    "        proposed_sample = uniform_rv(-10, 10)\n",
    "        acceptance_ratio = min(1, q(proposed_sample) / q(current_sample))\n",
    "    \n",
    "        # Accept or reject the proposed sample\n",
    "        if uniform_rv() < acceptance_ratio:\n",
    "            samples.append(proposed_sample)\n",
    "        else:\n",
    "            samples.append(current_sample) \n",
    "    return samples\n",
    "\n",
    "\n",
    "n_steps = 100000  # Number of MCMC steps\n",
    "samples = metropolis_hastings(n_steps, q = f3)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.hist(samples, bins = 50, density = True, label = 'M-H Samples')\n",
    "\n",
    "# Plotting the true density\n",
    "x_values = np.linspace(min(samples) if samples else 0, max(samples) if samples else 10, 1000)\n",
    "true_density = f3(x_values)\n",
    "plt.plot(x_values, true_density, 'r-', lw = 2, label = 'True Density')\n",
    "plt.title('Metropolis-Hastings Sampling')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b627b25",
   "metadata": {},
   "source": [
    "### 3.2.2 Sample from an Uniform Distribution (0.5 points)\n",
    "\n",
    "Now use the M-H algorithm to sample from an uniform distribution between 3 and 7  as your target, denoted here as `f4`. This is Problem 3 from [Hogg & Foreman-Mackey](https://iopscience-iop-org.tudelft.idm.oclc.org/article/10.3847/1538-4365/aab76e/meta).\n",
    "\n",
    "**Hint**: If you run into numerical instabilities, you need to replace the division with the difference of log probabilities in your M-H algorithm, as described by the last paragraph from Section 3 in [Hogg & Foreman-Mackey](https://iopscience-iop-org.tudelft.idm.oclc.org/article/10.3847/1538-4365/aab76e/meta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ab931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(x_value):\n",
    "    x_min = 3\n",
    "    x_max = 7\n",
    "    if x_min <= x_value and x_value <= x_max:\n",
    "        return 1 / (x_max - x_min) \n",
    "    return 0\n",
    "\n",
    "def metropolis_hastings(n_steps, q):\n",
    "    \"\"\"\n",
    "    Perform Metropolis-Hastings sampling to generate samples from the target distribution q.\n",
    "    Use the log of the target distribution to avoid underflow.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_steps (int): the number of MCMC steps.\n",
    "    - q (function): the target distribution.\n",
    "\n",
    "    Returns:\n",
    "    - list of floats: the generated samples.\n",
    "    \"\"\"\n",
    "    samples = [5]\n",
    "    # YOUR CODE HERE\n",
    "    for _ in range(1, n_steps):\n",
    "        current_sample = samples[-1]\n",
    "        proposed_sample = normal_rv(current_sample, np.sqrt(2))\n",
    "        acceptance_ratio = np.log(q(proposed_sample)) - np.log(q(current_sample))\n",
    "    \n",
    "        # Accept or reject the proposed sample\n",
    "        if np.log(uniform_rv()) < acceptance_ratio:\n",
    "            samples.append(proposed_sample)\n",
    "        else:\n",
    "            samples.append(current_sample) \n",
    "    return samples\n",
    "\n",
    "n_steps = 500000  # Number of MCMC steps\n",
    "samples = metropolis_hastings(n_steps, q = f4)\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.hist(samples, bins = 50, density = True, label = \"M-H Samples\")\n",
    "\n",
    "# Plotting the true density\n",
    "x_values = np.linspace(0, 10, 1000)\n",
    "true_density = [f4(x) for x in x_values]\n",
    "plt.plot(x_values, true_density, 'r-', lw = 2, label='True Density')\n",
    "plt.title('Metropolis-Hastings Sampling')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee52a221",
   "metadata": {},
   "source": [
    "### 3.3 Integral using the M-H algorithm (1 point)\n",
    "\n",
    "Now that you have a working M-H sampler, we can use it to calculate the integral of `fun` given a target density `target`.\n",
    "\n",
    "**Hint:** Instead of sampling directly from the target density, you will use the M-H algorithm to sample from the `target` density and then calculate the integral using the importance sampling formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b30ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_integral_MH(fun, target, n_steps=500_000):\n",
    "    \"\"\"\n",
    "    Calculate the integral of a function using the Metropolis-Hastings algorithm for sampling\n",
    "\n",
    "    Parameters:\n",
    "    - fun (function): the function to integrate.\n",
    "    - target (function): the target distribution.\n",
    "\n",
    "    Returns:\n",
    "    - float: the estimated value of the integral.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    samples = metropolis_hastings(n_steps, target)\n",
    "    importance_weights = np.array([fun(s) / target(s) for s in samples])\n",
    "    integral_estimate = np.mean(importance_weights)\n",
    "    return integral_estimate\n",
    "\n",
    "approximation = calculate_integral_MH(f1, f3)\n",
    "print(f\"Your approximation : {approximation}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "612d4f25",
   "metadata": {},
   "source": [
    "We can test the function by integrating `f3` and `f4` from the previous exercises, using them as both the target function and the target density distribution. The result of the integrals should be `1`, as the integral of a probability density function over its domain is always one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9655446",
   "metadata": {},
   "outputs": [],
   "source": [
    "approximation = calculate_integral_MH(f3, f3) # approximate the integral of f3 using f3 as the target distribution\n",
    "result = 1 # The integral of f3 is 1\n",
    "print(f\"Your approximation : {approximation}; actual answer: {result}; error :{np.abs(result - approximation)}\")\n",
    "approximation = calculate_integral_MH(f4, f4)\n",
    "print(f\"Your approximation : {approximation}; actual answer: {result}; error :{np.abs(result - approximation)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3506b43a",
   "metadata": {},
   "source": [
    "We can also test the function by integrating $f_1(x) = x^2$ over the interval defined by $f_4(x) = 1/4$ for  $x \\in [3,7]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "approximation = calculate_integral_MH(f1, f4) # approximate the integral of f1 using f4 as the target distribution\n",
    "result = f1_integral(3, 7) # actual answer\n",
    "print(f\"Your approximation : {approximation}; actual answer: {result}; error :{np.abs(result - approximation)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff9e6a74",
   "metadata": {},
   "source": [
    "To use `calculate_integral_MH` on any function, we need to adapt the function to accept a single argument, `x`. This can be accomplished through the use of a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129326d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_lambda(a, mu, sigma):\n",
    "    return lambda x : a * np.exp(-(x - mu) ** 2 / (2 * sigma ** 2))\n",
    "\n",
    "def f2_pdf(mu, sigma):\n",
    "    return lambda x : normal_pdf(x, mu, sigma ** 2)\n",
    "\n",
    "def f2_integral(a, mu, sigma):\n",
    "    return a * np.sqrt(2 * np.pi) * sigma\n",
    "\n",
    "a = 3\n",
    "mu = 1\n",
    "sigma = 2\n",
    "approximation = calculate_integral_MH(f2_lambda(a, mu, sigma), f2_pdf(mu, sigma))\n",
    "result = f2_integral(a, mu, sigma)\n",
    "print(f\"Your approximation : {approximation}; actual answer: {result}; error :{np.abs(result - approximation)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
